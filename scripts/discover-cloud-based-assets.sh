#!/bin/bash
#############################################################################################
# Script to find, as much as possible, Cloud based services (AWS/AZURE/GCP) 
# used by a company based on its main web domain, as input information.
#
# Based on the following project for AZURE information:
# 	https://github.com/NetSPI/MicroBurst/blob/master/Misc/Invoke-EnumerateAzureSubDomains.ps
#
# Requirements in terms of software:
#	https://github.com/iknowjason/edge
# 	https://github.com/projectdiscovery/httpx
# 	apt install wget curl jq dnsutils whois nmap
# 
# Note:
#	System command was used, for DNS related work, because it was more reliable than 
#	"github.com/projectdiscovery/dnsx" when the container was used within the 
#	context of an VPN connection on the docker host.
#
# Remark:
#	Focus was made on reliability of data retrieved over the performance aspects.
#############################################################################################

# Constants
INIT_MARKER_FILE="/tmp/$(basename $0).init"
PERMUTATIONS_FILE="/tmp/permutations.txt"
CRTSH_DATA_FILE="/tmp/crtshdata.json"
WORK_FILE="/tmp/work.txt"
CSV_FILE="assets.csv"
CSV_FILE_HEADERS="Provider,Subdomain,IPv4"
THREAD_COUNT=10
GO_BIN_HOME="/root/go/bin"
EDGE_HOME="/tools/edge"
EDGE_TRACE_FILE="edge_trace.txt"
AZURE_SUBDOMAINS=("onmicrosoft.com" "scm.azurewebsites.net" "azurewebsites.net" "p.azurewebsites.net" "cloudapp.net" "file.core.windows.net" "blob.core.windows.net" "queue.core.windows.net" "table.core.windows.net" "mail.protection.outlook.com" "sharepoint.com" "redis.cache.windows.net" "documents.azure.com" "database.windows.net" "vault.azure.net" "azureedge.net" "search.windows.net" "azure-api.net" "azurecr.io")
EXTRA_PROVIDERS_SUBDOMAINS=("service-now.com")
PROVIDERS_SUBDOMAINS=("${AZURE_SUBDOMAINS[@]}" "${EXTRA_PROVIDERS_SUBDOMAINS[@]}")

# Entry point
if [ "$#" -lt 1 ]; then
    script_name=$(basename "$0")
    echo "Usage:"
    echo "   $script_name [COMPANY_BASE_DOMAIN]"
    echo ""
    echo "Call example:"
    echo "    $script_name righettod.eu"
    exit 1
fi

# Utility functions
function write_step(){
    echo -e "\e[93m$1\e[0m"
}

function extract_dns_san_from_tls_cert(){
	subdomain=$1
	#san_entries=$(openssl s_client -connect $subdomain:443 </dev/null 2>/dev/null | openssl x509 -noout -text | grep -E "DNS:(.*)" | sed 's/DNS://g' | tr -d ' ')
	san_entries=$(nmap -Pn -p 443 --script ssl-cert $subdomain 2>/dev/null | grep -F "Subject Alternative Name:" | sed 's/Subject Alternative Name://g' | cut -d'|' -f2 | tr -d " " | sed 's/DNS://g')
	if [ ${#san_entries} -gt 1 ]
	then
		for san_entry in $(echo $san_entries | tr "," "\n")
		do
			is_star_san=$(echo $san_entry | grep -Fc "*")
			if [ "is_star_san" == "0" ]
			then
				process_subdomain $san_entry
			fi
		done
	fi
	
}

function process_subdomain(){
	subdomain=$1
	for ipv4 in $(dig +time=2 +tries=2 +retry=1 +short A $subdomain)
	do
		is_ipv4=$(echo "$ipv4" | grep -Ec "^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$")
		if [ $is_ipv4 -ne 1 ]
		then
			continue
		fi
		whois_record=$(whois $ipv4 2>/dev/null)
		is_azure=$(echo "$whois_record" | grep -iFc "MICROSOFT")
		is_aws=$(echo "$whois_record" | grep -iFc "AMAZON")
		is_gcp=$(echo "$whois_record" | grep -iFc "GOOGLE")
		provider="NONE"
		if [ $is_azure -gt 0 ]
		then
			provider="AZURE"
		elif [ $is_aws -gt 0 ]
		then
			provider="AWS"
		elif [ $is_gcp -gt 0 ]
		then
			provider="GCP"			
		fi
		if [ "$provider" != "NONE" ]
		then
			print_result_line "$provider" "$subdomain" "$ipv4"
		fi		
	done	
}

function print_result_line(){
	provider=$1
	subdomain=$2
	ipv4=$3	
	printf "[%-5s] %-15s => %s\n" "$provider" "$ipv4" "$subdomain"
	echo "$provider,$subdomain,$ipv4" >> $CSV_FILE
}


# Working context
BASE_DOMAIN="$1"
BASE=$(echo $BASE_DOMAIN | cut -d'.' -f1)

# Main processing
rm $CSV_FILE 2>/dev/null
write_step "[Discovery] Download cloud providers and sub domains data for domain '$BASE_DOMAIN' using base as '$BASE'..."
curl -sk --output $CRTSH_DATA_FILE "https://crt.sh/?q=%25.$BASE_DOMAIN&output=json"
validity_check=$(grep -Fc "$BASE_DOMAIN" $CRTSH_DATA_FILE)
if [ $validity_check -eq 0 ]
then
	echo "File with certificate transparency records does not contains '$BASE_DOMAIN', it can due to 'crt.sh' overload: Please retry."
	exit 1
fi
if [ ! -f $INIT_MARKER_FILE ]
then 
	curl -sk https://raw.githubusercontent.com/NetSPI/MicroBurst/master/Misc/permutations.txt | sed 's/\$//g' > $PERMUTATIONS_FILE
	curl -sk https://azureipranges.azurewebsites.net/Home/Update > /dev/null
	wget -q -O $EDGE_HOME/ip-ranges.json https://ip-ranges.amazonaws.com/ip-ranges.json
	wget -q -O $EDGE_HOME/goog.json https://www.gstatic.com/ipranges/goog.json
	wget -q -O $EDGE_HOME/azure.json https://azureipranges.azurewebsites.net/Data/Public.json
	echo "X" > $INIT_MARKER_FILE
	echo "Done."
else
	echo "Already performed."
fi
ls -l $PERMUTATIONS_FILE $CRTSH_DATA_FILE $EDGE_HOME/*.json
write_step "[Discovery] AZURE and other services via DNS discovery..."
for sub_domain in "${PROVIDERS_SUBDOMAINS[@]}"
do
	# Direct case
	subdomain="$BASE.$sub_domain"
	process_subdomain $subdomain
	while IFS= read -r word
	do
		# Prefix case
		subdomain="$word$BASE.$sub_domain"
		process_subdomain $subdomain
		# Suffix case	
		subdomain="$BASE$word.$sub_domain"
		process_subdomain $subdomain		
	done < "$PERMUTATIONS_FILE"
done
write_step "[Discovery] AWS S3 buckets via HTTP discovery..."
aws_s3_domain="s3.amazonaws.com"
# Direct case
subdomain="$BASE.$aws_s3_domain"
echo $subdomain > $WORK_FILE
while IFS= read -r word
do
	# Prefix case
	subdomain="$word$BASE.$aws_s3_domain"
	echo $subdomain >> $WORK_FILE
	# Suffix case	
	subdomain="$BASE$word.$aws_s3_domain"
	echo $subdomain >> $WORK_FILE		
done < "$PERMUTATIONS_FILE"
$GO_BIN_HOME/httpx  -duc -silent -fe "(NoSuchBucket|IllegalLocationConstraintException)" -title -t $THREAD_COUNT -rl $THREAD_COUNT -list $WORK_FILE -json > $WORK_FILE.tmp
mv $WORK_FILE.tmp $WORK_FILE
for line in $(cat $WORK_FILE)
do
	print_result_line "AWS" "$(echo $line | jq -r '.input')" "$(echo $line | jq -r '.host')"
done
write_step "[Discovery] AZURE/AWS/GCP services via 'crt.sh' entries (CN entries)..."
cat $CRTSH_DATA_FILE | jq -r ".[].common_name" | sort -u | grep -iv "\.local$" | grep -Fv "*" > $WORK_FILE
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Discovery] AZURE/AWS/GCP services via 'crt.sh' entries (SAN entries)..."
while IFS= read -r subdomain
do
	extract_dns_san_from_tls_cert $subdomain
done < "$WORK_FILE"
write_step "[Discovery] AZURE/AWS/GCP services via the 'EDGE' tools..."
echo "[i] See file '$EDGE_TRACE_FILE' for Edge raw results in case of later need."
cdir=$(pwd)
cd $EDGE_HOME
./edge -domain "$BASE_DOMAIN" -nd -dns -crt -silent > $WORK_FILE
cd $cdir
cp $WORK_FILE $EDGE_TRACE_FILE
cat $WORK_FILE | grep -F ",A," | cut -d',' -f1 | sort -u > $WORK_FILE.tmp
mv $WORK_FILE.tmp $WORK_FILE
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Gathering] Assemble the final CSV file '$CSV_FILE'..."
echo $CSV_FILE_HEADERS > "$WORK_FILE"
cat $CSV_FILE | sort -u >> $WORK_FILE
mv $WORK_FILE $CSV_FILE
rm $CRTSH_DATA_FILE
